{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38847b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import  pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da5f5183",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('Admission_Predict.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06d091ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "395         396        324          110                  3  3.5   3.5  9.04   \n",
       "396         397        325          107                  3  3.0   3.5  9.11   \n",
       "397         398        330          116                  4  5.0   4.5  9.45   \n",
       "398         399        312          103                  3  3.5   4.0  8.78   \n",
       "399         400        333          117                  4  5.0   4.0  9.66   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "395         1              0.82  \n",
       "396         1              0.84  \n",
       "397         1              0.91  \n",
       "398         0              0.67  \n",
       "399         1              0.95  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c71aa61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         400 non-null    int64  \n",
      " 1   GRE Score          400 non-null    int64  \n",
      " 2   TOEFL Score        400 non-null    int64  \n",
      " 3   University Rating  400 non-null    int64  \n",
      " 4   SOP                400 non-null    float64\n",
      " 5   LOR                400 non-null    float64\n",
      " 6   CGPA               400 non-null    float64\n",
      " 7   Research           400 non-null    int64  \n",
      " 8   Chance of Admit    400 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 28.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c746cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e836d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "395        324          110                  3  3.5   3.5  9.04         1   \n",
       "396        325          107                  3  3.0   3.5  9.11         1   \n",
       "397        330          116                  4  5.0   4.5  9.45         1   \n",
       "398        312          103                  3  3.5   4.0  8.78         0   \n",
       "399        333          117                  4  5.0   4.0  9.66         1   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "395              0.82  \n",
       "396              0.84  \n",
       "397              0.91  \n",
       "398              0.67  \n",
       "399              0.95  \n",
       "\n",
       "[400 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['Serial No.'],inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e1f1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eca0162d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0        337          118                  4  4.5   4.5  9.65         1\n",
       "1        324          107                  4  4.0   4.5  8.87         1\n",
       "2        316          104                  3  3.0   3.5  8.00         1\n",
       "3        322          110                  3  3.5   2.5  8.67         1\n",
       "4        314          103                  2  2.0   3.0  8.21         0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b06dfc09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.92\n",
       "1    0.76\n",
       "2    0.72\n",
       "3    0.80\n",
       "4    0.65\n",
       "Name: Chance of Admit , dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=df.iloc[:,-1]\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07c6281d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bca5b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f55f460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22      , 0.17857143, 0.25      , ..., 0.42857143, 0.25      ,\n",
       "        1.        ],\n",
       "       [0.88      , 0.96428571, 1.        , ..., 0.85714286, 0.91911765,\n",
       "        1.        ],\n",
       "       [0.3       , 0.71428571, 0.5       , ..., 0.57142857, 0.53308824,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.7       , 0.53571429, 0.5       , ..., 0.57142857, 0.70220588,\n",
       "        1.        ],\n",
       "       [0.72      , 0.67857143, 1.        , ..., 0.71428571, 0.74632353,\n",
       "        1.        ],\n",
       "       [0.2       , 0.46428571, 0.        , ..., 0.14285714, 0.22058824,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de29f737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44      ,  0.39285714,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.58088235,  0.        ],\n",
       "       [ 0.2       ,  0.28571429,  0.5       ,  0.25      ,  0.42857143,\n",
       "         0.53676471,  1.        ],\n",
       "       [ 0.68      ,  0.71428571,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.57720588,  1.        ],\n",
       "       [ 0.68      ,  0.53571429,  1.        ,  0.625     ,  0.71428571,\n",
       "         0.53676471,  1.        ],\n",
       "       [ 0.64      ,  0.64285714,  0.75      ,  0.75      ,  1.        ,\n",
       "         0.70955882,  1.        ],\n",
       "       [ 0.36      ,  0.5       ,  0.5       ,  0.5       ,  0.42857143,\n",
       "         0.38235294,  0.        ],\n",
       "       [ 0.4       ,  0.5       ,  0.25      ,  0.625     ,  0.28571429,\n",
       "         0.41544118,  0.        ],\n",
       "       [ 0.2       ,  0.35714286,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.24632353,  0.        ],\n",
       "       [ 0.4       ,  0.25      ,  0.25      ,  0.125     ,  0.14285714,\n",
       "         0.03676471,  0.        ],\n",
       "       [ 1.        ,  0.71428571,  0.75      ,  1.        ,  0.85714286,\n",
       "         0.90441176,  1.        ],\n",
       "       [ 0.62      ,  0.67857143,  0.75      ,  0.75      ,  0.71428571,\n",
       "         0.65073529,  1.        ],\n",
       "       [ 0.92      ,  0.96428571,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.88970588,  1.        ],\n",
       "       [ 0.82      ,  0.82142857,  1.        ,  0.875     ,  0.57142857,\n",
       "         0.79411765,  1.        ],\n",
       "       [ 0.5       ,  0.42857143,  0.5       ,  0.5       ,  0.28571429,\n",
       "         0.41544118,  0.        ],\n",
       "       [ 0.66      ,  0.42857143,  0.5       ,  0.75      ,  0.71428571,\n",
       "         0.45588235,  1.        ],\n",
       "       [ 0.44      ,  0.64285714,  0.25      ,  0.625     ,  0.42857143,\n",
       "         0.48897059,  0.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.74264706,  1.        ],\n",
       "       [ 0.8       ,  0.82142857,  1.        ,  0.875     ,  0.42857143,\n",
       "         0.78676471,  1.        ],\n",
       "       [ 0.12      ,  0.10714286,  0.25      ,  0.5       ,  0.14285714,\n",
       "         0.125     ,  1.        ],\n",
       "       [ 0.36      ,  0.60714286,  0.25      ,  0.5       ,  0.71428571,\n",
       "         0.45955882,  0.        ],\n",
       "       [ 0.4       ,  0.5       ,  0.75      ,  0.125     ,  0.28571429,\n",
       "         0.42647059,  0.        ],\n",
       "       [ 0.68      ,  0.82142857,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.57352941,  1.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.71691176,  1.        ],\n",
       "       [ 0.82      ,  0.89285714,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.81617647,  1.        ],\n",
       "       [ 0.58      ,  0.5       ,  0.5       ,  0.75      ,  0.42857143,\n",
       "         0.29411765,  1.        ],\n",
       "       [ 0.16      ,  0.21428571,  0.25      ,  0.75      ,  0.42857143,\n",
       "         0.30514706,  0.        ],\n",
       "       [ 0.6       ,  0.42857143,  0.5       ,  0.625     ,  0.85714286,\n",
       "         0.41911765,  1.        ],\n",
       "       [ 0.5       ,  0.64285714,  0.25      ,  0.625     ,  0.42857143,\n",
       "         0.46323529,  1.        ],\n",
       "       [ 0.88      ,  0.85714286,  0.75      ,  0.75      ,  0.57142857,\n",
       "         0.86029412,  1.        ],\n",
       "       [ 0.4       ,  0.35714286,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.30147059,  1.        ],\n",
       "       [ 1.        ,  1.        ,  0.75      ,  1.        ,  1.        ,\n",
       "         0.84558824,  1.        ],\n",
       "       [ 0.48      ,  0.39285714,  0.25      ,  0.25      ,  0.42857143,\n",
       "         0.37132353,  0.        ],\n",
       "       [ 0.64      ,  0.64285714,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.65073529,  0.        ],\n",
       "       [ 0.64      ,  0.64285714,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.64705882,  1.        ],\n",
       "       [ 0.62      ,  0.60714286,  0.5       ,  0.5       ,  0.71428571,\n",
       "         0.36764706,  1.        ],\n",
       "       [ 0.44      ,  0.46428571,  0.5       ,  0.25      ,  0.42857143,\n",
       "         0.30147059,  1.        ],\n",
       "       [ 0.2       ,  0.25      ,  0.        ,  0.5       ,  0.14285714,\n",
       "        -0.14705882,  1.        ],\n",
       "       [ 0.48      ,  0.5       ,  0.25      ,  0.75      ,  0.57142857,\n",
       "         0.38602941,  0.        ],\n",
       "       [ 0.8       ,  0.85714286,  0.75      ,  1.        ,  0.85714286,\n",
       "         0.82720588,  1.        ],\n",
       "       [ 0.56      ,  0.60714286,  0.        ,  0.625     ,  0.57142857,\n",
       "         0.70588235,  0.        ],\n",
       "       [ 0.74      ,  0.75      ,  0.75      ,  0.875     ,  1.        ,\n",
       "         0.71323529,  0.        ],\n",
       "       [ 0.16      ,  0.46428571,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.49264706,  0.        ],\n",
       "       [ 0.88      ,  0.89285714,  1.        ,  0.75      ,  0.85714286,\n",
       "         0.6875    ,  1.        ],\n",
       "       [ 0.28      ,  0.46428571,  0.25      ,  0.5       ,  0.42857143,\n",
       "         0.36764706,  1.        ],\n",
       "       [ 0.9       ,  0.89285714,  1.        ,  1.        ,  1.        ,\n",
       "         0.96323529,  1.        ],\n",
       "       [ 0.58      ,  0.39285714,  0.75      ,  0.875     ,  0.57142857,\n",
       "         0.53676471,  0.        ],\n",
       "       [ 0.6       ,  0.57142857,  0.5       ,  0.625     ,  0.71428571,\n",
       "         0.45588235,  1.        ],\n",
       "       [ 0.7       ,  0.71428571,  0.75      ,  0.75      ,  0.71428571,\n",
       "         0.66176471,  1.        ],\n",
       "       [ 0.3       ,  0.46428571,  0.25      ,  0.5       ,  0.71428571,\n",
       "         0.34191176,  0.        ],\n",
       "       [ 0.36      ,  0.39285714,  0.25      ,  0.5       ,  0.57142857,\n",
       "         0.47426471,  0.        ],\n",
       "       [ 0.7       ,  0.71428571,  0.75      ,  0.625     ,  0.57142857,\n",
       "         0.63235294,  0.        ],\n",
       "       [ 0.12      ,  0.25      ,  0.25      ,  0.375     ,  0.28571429,\n",
       "         0.30514706,  0.        ],\n",
       "       [ 0.68      ,  0.67857143,  0.75      ,  0.5       ,  0.42857143,\n",
       "         0.66544118,  1.        ],\n",
       "       [ 0.56      ,  0.64285714,  0.5       ,  0.75      ,  0.42857143,\n",
       "         0.58823529,  0.        ],\n",
       "       [ 0.4       ,  0.53571429,  0.5       ,  0.625     ,  0.57142857,\n",
       "         0.54044118,  0.        ],\n",
       "       [ 0.92      ,  0.92857143,  1.        ,  0.875     ,  0.71428571,\n",
       "         0.73161765,  1.        ],\n",
       "       [ 0.08      ,  0.10714286,  0.        ,  0.125     ,  0.        ,\n",
       "         0.16176471,  0.        ],\n",
       "       [ 0.        ,  0.42857143,  0.75      ,  0.25      ,  0.28571429,\n",
       "         0.09558824,  0.        ],\n",
       "       [ 0.72      ,  0.35714286,  0.75      ,  1.        ,  1.        ,\n",
       "         0.57352941,  1.        ],\n",
       "       [ 0.64      ,  0.64285714,  0.5       ,  0.75      ,  1.        ,\n",
       "         0.52941176,  1.        ],\n",
       "       [ 0.96      ,  0.89285714,  0.75      ,  0.625     ,  0.85714286,\n",
       "         0.83088235,  1.        ],\n",
       "       [ 0.52      ,  0.53571429,  0.25      ,  0.625     ,  0.57142857,\n",
       "         0.52941176,  1.        ],\n",
       "       [ 0.8       ,  0.75      ,  1.        ,  1.        ,  0.71428571,\n",
       "         0.77573529,  1.        ],\n",
       "       [ 0.52      ,  0.46428571,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.5625    ,  0.        ],\n",
       "       [ 0.14      ,  0.21428571,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.17279412,  0.        ],\n",
       "       [ 0.68      ,  0.75      ,  0.75      ,  0.875     ,  0.71428571,\n",
       "         0.58455882,  0.        ],\n",
       "       [ 0.78      ,  0.67857143,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.72794118,  1.        ],\n",
       "       [ 0.6       ,  0.42857143,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.56617647,  1.        ],\n",
       "       [ 0.32      ,  0.46428571,  0.25      ,  0.375     ,  0.42857143,\n",
       "         0.375     ,  1.        ],\n",
       "       [ 0.42      ,  0.53571429,  0.75      ,  0.875     ,  0.85714286,\n",
       "         0.66176471,  1.        ],\n",
       "       [ 0.68      ,  0.64285714,  0.75      ,  0.5       ,  0.57142857,\n",
       "         0.65073529,  1.        ],\n",
       "       [ 0.28      ,  0.28571429,  0.75      ,  0.125     ,  0.28571429,\n",
       "         0.23529412,  0.        ],\n",
       "       [ 0.56      ,  0.53571429,  0.5       ,  0.5       ,  0.57142857,\n",
       "         0.39338235,  1.        ],\n",
       "       [ 0.34      ,  0.46428571,  0.25      ,  0.25      ,  0.57142857,\n",
       "         0.33088235,  0.        ],\n",
       "       [ 0.74      ,  0.67857143,  0.75      ,  0.75      ,  0.85714286,\n",
       "         0.66176471,  1.        ],\n",
       "       [ 0.18      ,  0.07142857,  0.        ,  0.        , -0.14285714,\n",
       "         0.05147059,  0.        ],\n",
       "       [ 0.52      ,  0.46428571,  0.25      ,  0.375     ,  0.28571429,\n",
       "         0.36764706,  1.        ],\n",
       "       [ 0.2       ,  0.42857143,  0.5       ,  0.625     ,  0.42857143,\n",
       "         0.35294118,  0.        ],\n",
       "       [ 0.38      ,  0.46428571,  1.        ,  0.625     ,  0.57142857,\n",
       "         0.5       ,  0.        ],\n",
       "       [ 0.3       ,  0.46428571,  0.25      ,  0.5       ,  0.14285714,\n",
       "         0.37867647,  0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bc1544f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "2f2582db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\okok\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(7,activation='relu',input_dim=7)) #no. of nodes on hidden layer=7, input_dim i.e input layer node=7\n",
    "model.add(Dense(7,activation='relu')) \n",
    "# model.add(Dense(3,activation='relu')) \n",
    "model.add(Dense(1,activation='linear')) # if working on regression model make sure that the o/p layer should have activation='linear'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "02d5528c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_8\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_8\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │              \u001b[38;5;34m56\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │              \u001b[38;5;34m56\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m8\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fc3f4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error',optimizer='Adam' ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "07633686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 38ms/step - accuracy: 0.0000e+00 - loss: 0.2980 - val_accuracy: 0.0000e+00 - val_loss: 0.2686\n",
      "Epoch 2/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.2345 - val_accuracy: 0.0000e+00 - val_loss: 0.1985\n",
      "Epoch 3/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.1747 - val_accuracy: 0.0000e+00 - val_loss: 0.1386\n",
      "Epoch 4/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.1256 - val_accuracy: 0.0000e+00 - val_loss: 0.0913\n",
      "Epoch 5/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0751 - val_accuracy: 0.0000e+00 - val_loss: 0.0560\n",
      "Epoch 6/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0504 - val_accuracy: 0.0000e+00 - val_loss: 0.0352\n",
      "Epoch 7/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0339 - val_accuracy: 0.0000e+00 - val_loss: 0.0301\n",
      "Epoch 8/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0292 - val_accuracy: 0.0000e+00 - val_loss: 0.0292\n",
      "Epoch 9/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0282 - val_accuracy: 0.0000e+00 - val_loss: 0.0260\n",
      "Epoch 10/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0261 - val_accuracy: 0.0000e+00 - val_loss: 0.0230\n",
      "Epoch 11/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0253 - val_accuracy: 0.0000e+00 - val_loss: 0.0214\n",
      "Epoch 12/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0258 - val_accuracy: 0.0000e+00 - val_loss: 0.0198\n",
      "Epoch 13/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0214 - val_accuracy: 0.0000e+00 - val_loss: 0.0184\n",
      "Epoch 14/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0186 - val_accuracy: 0.0000e+00 - val_loss: 0.0172\n",
      "Epoch 15/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0176 - val_accuracy: 0.0000e+00 - val_loss: 0.0162\n",
      "Epoch 16/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 0.0178 - val_accuracy: 0.0000e+00 - val_loss: 0.0154\n",
      "Epoch 17/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0163 - val_accuracy: 0.0000e+00 - val_loss: 0.0148\n",
      "Epoch 18/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0168 - val_accuracy: 0.0000e+00 - val_loss: 0.0143\n",
      "Epoch 19/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0163 - val_accuracy: 0.0000e+00 - val_loss: 0.0139\n",
      "Epoch 20/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.0140 - val_accuracy: 0.0000e+00 - val_loss: 0.0136\n",
      "Epoch 21/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0158 - val_accuracy: 0.0000e+00 - val_loss: 0.0132\n",
      "Epoch 22/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0139 - val_accuracy: 0.0000e+00 - val_loss: 0.0129\n",
      "Epoch 23/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0171 - val_accuracy: 0.0000e+00 - val_loss: 0.0126\n",
      "Epoch 24/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0144 - val_accuracy: 0.0000e+00 - val_loss: 0.0123\n",
      "Epoch 25/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0150 - val_accuracy: 0.0000e+00 - val_loss: 0.0120\n",
      "Epoch 26/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0142 - val_accuracy: 0.0000e+00 - val_loss: 0.0118\n",
      "Epoch 27/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0131 - val_accuracy: 0.0000e+00 - val_loss: 0.0115\n",
      "Epoch 28/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0136 - val_accuracy: 0.0000e+00 - val_loss: 0.0113\n",
      "Epoch 29/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0127 - val_accuracy: 0.0000e+00 - val_loss: 0.0110\n",
      "Epoch 30/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0129 - val_accuracy: 0.0000e+00 - val_loss: 0.0107\n",
      "Epoch 31/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0000e+00 - loss: 0.0120 - val_accuracy: 0.0000e+00 - val_loss: 0.0105\n",
      "Epoch 32/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0114 - val_accuracy: 0.0000e+00 - val_loss: 0.0102\n",
      "Epoch 33/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0114 - val_accuracy: 0.0000e+00 - val_loss: 0.0100\n",
      "Epoch 34/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0107 - val_accuracy: 0.0000e+00 - val_loss: 0.0097\n",
      "Epoch 35/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0107 - val_accuracy: 0.0000e+00 - val_loss: 0.0095\n",
      "Epoch 36/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0113 - val_accuracy: 0.0000e+00 - val_loss: 0.0092\n",
      "Epoch 37/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0097 - val_accuracy: 0.0000e+00 - val_loss: 0.0091\n",
      "Epoch 38/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0105 - val_accuracy: 0.0000e+00 - val_loss: 0.0089\n",
      "Epoch 39/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0106 - val_accuracy: 0.0000e+00 - val_loss: 0.0088\n",
      "Epoch 40/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0089 - val_accuracy: 0.0000e+00 - val_loss: 0.0085\n",
      "Epoch 41/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0085 - val_accuracy: 0.0000e+00 - val_loss: 0.0084\n",
      "Epoch 42/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0091 - val_accuracy: 0.0000e+00 - val_loss: 0.0082\n",
      "Epoch 43/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0093 - val_accuracy: 0.0000e+00 - val_loss: 0.0082\n",
      "Epoch 44/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0091 - val_accuracy: 0.0000e+00 - val_loss: 0.0080\n",
      "Epoch 45/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0099 - val_accuracy: 0.0000e+00 - val_loss: 0.0079\n",
      "Epoch 46/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0094 - val_accuracy: 0.0000e+00 - val_loss: 0.0078\n",
      "Epoch 47/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0080 - val_accuracy: 0.0000e+00 - val_loss: 0.0077\n",
      "Epoch 48/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0092 - val_accuracy: 0.0000e+00 - val_loss: 0.0077\n",
      "Epoch 49/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0076 - val_accuracy: 0.0000e+00 - val_loss: 0.0075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0080 - val_accuracy: 0.0000e+00 - val_loss: 0.0075\n",
      "Epoch 51/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0088 - val_accuracy: 0.0000e+00 - val_loss: 0.0074\n",
      "Epoch 52/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0078 - val_accuracy: 0.0000e+00 - val_loss: 0.0075\n",
      "Epoch 53/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0079 - val_accuracy: 0.0000e+00 - val_loss: 0.0073\n",
      "Epoch 54/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0077 - val_accuracy: 0.0000e+00 - val_loss: 0.0072\n",
      "Epoch 55/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0079 - val_accuracy: 0.0000e+00 - val_loss: 0.0072\n",
      "Epoch 56/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0083 - val_accuracy: 0.0000e+00 - val_loss: 0.0072\n",
      "Epoch 57/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0072 - val_accuracy: 0.0000e+00 - val_loss: 0.0072\n",
      "Epoch 58/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0067 - val_accuracy: 0.0000e+00 - val_loss: 0.0070\n",
      "Epoch 59/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0071 - val_accuracy: 0.0000e+00 - val_loss: 0.0071\n",
      "Epoch 60/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0066 - val_accuracy: 0.0000e+00 - val_loss: 0.0069\n",
      "Epoch 61/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0069 - val_accuracy: 0.0000e+00 - val_loss: 0.0069\n",
      "Epoch 62/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0070 - val_accuracy: 0.0000e+00 - val_loss: 0.0069\n",
      "Epoch 63/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0072 - val_accuracy: 0.0000e+00 - val_loss: 0.0068\n",
      "Epoch 64/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0067 - val_accuracy: 0.0000e+00 - val_loss: 0.0068\n",
      "Epoch 65/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0066 - val_accuracy: 0.0000e+00 - val_loss: 0.0068\n",
      "Epoch 66/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0061 - val_accuracy: 0.0000e+00 - val_loss: 0.0066\n",
      "Epoch 67/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0060 - val_accuracy: 0.0000e+00 - val_loss: 0.0067\n",
      "Epoch 68/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0061 - val_accuracy: 0.0000e+00 - val_loss: 0.0067\n",
      "Epoch 69/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0067 - val_accuracy: 0.0000e+00 - val_loss: 0.0064\n",
      "Epoch 70/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0057 - val_accuracy: 0.0000e+00 - val_loss: 0.0064\n",
      "Epoch 71/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0056 - val_accuracy: 0.0000e+00 - val_loss: 0.0066\n",
      "Epoch 72/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0071 - val_accuracy: 0.0000e+00 - val_loss: 0.0065\n",
      "Epoch 73/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0074 - val_accuracy: 0.0000e+00 - val_loss: 0.0064\n",
      "Epoch 74/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0061 - val_accuracy: 0.0000e+00 - val_loss: 0.0062\n",
      "Epoch 75/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0070 - val_accuracy: 0.0000e+00 - val_loss: 0.0063\n",
      "Epoch 76/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0062 - val_accuracy: 0.0000e+00 - val_loss: 0.0063\n",
      "Epoch 77/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0067 - val_accuracy: 0.0000e+00 - val_loss: 0.0061\n",
      "Epoch 78/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0063 - val_accuracy: 0.0000e+00 - val_loss: 0.0063\n",
      "Epoch 79/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0061 - val_accuracy: 0.0000e+00 - val_loss: 0.0062\n",
      "Epoch 80/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0059 - val_accuracy: 0.0000e+00 - val_loss: 0.0061\n",
      "Epoch 81/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0062 - val_accuracy: 0.0000e+00 - val_loss: 0.0062\n",
      "Epoch 82/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0058 - val_accuracy: 0.0000e+00 - val_loss: 0.0061\n",
      "Epoch 83/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0055 - val_accuracy: 0.0000e+00 - val_loss: 0.0059\n",
      "Epoch 84/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0049 - val_accuracy: 0.0000e+00 - val_loss: 0.0060\n",
      "Epoch 85/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - accuracy: 0.0000e+00 - loss: 0.0059 - val_accuracy: 0.0000e+00 - val_loss: 0.0060\n",
      "Epoch 86/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - accuracy: 0.0000e+00 - loss: 0.0059 - val_accuracy: 0.0000e+00 - val_loss: 0.0060\n",
      "Epoch 87/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0061 - val_accuracy: 0.0000e+00 - val_loss: 0.0058\n",
      "Epoch 88/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0059 - val_accuracy: 0.0000e+00 - val_loss: 0.0058\n",
      "Epoch 89/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0059 - val_accuracy: 0.0000e+00 - val_loss: 0.0059\n",
      "Epoch 90/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0065 - val_accuracy: 0.0000e+00 - val_loss: 0.0059\n",
      "Epoch 91/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0056 - val_accuracy: 0.0000e+00 - val_loss: 0.0058\n",
      "Epoch 92/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0056 - val_accuracy: 0.0000e+00 - val_loss: 0.0057\n",
      "Epoch 93/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0061 - val_accuracy: 0.0000e+00 - val_loss: 0.0058\n",
      "Epoch 94/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0057 - val_accuracy: 0.0000e+00 - val_loss: 0.0057\n",
      "Epoch 95/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0057 - val_accuracy: 0.0000e+00 - val_loss: 0.0057\n",
      "Epoch 96/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0048 - val_accuracy: 0.0000e+00 - val_loss: 0.0058\n",
      "Epoch 97/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.0000e+00 - loss: 0.0057 - val_accuracy: 0.0000e+00 - val_loss: 0.0057\n",
      "Epoch 98/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0053 - val_accuracy: 0.0000e+00 - val_loss: 0.0056\n",
      "Epoch 99/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.0000e+00 - loss: 0.0053 - val_accuracy: 0.0000e+00 - val_loss: 0.0058\n",
      "Epoch 100/100\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.0000e+00 - loss: 0.0053 - val_accuracy: 0.0000e+00 - val_loss: 0.0056\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled,Y_train,epochs=100,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3238afa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "50f51987",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7236287857841913"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "16ee16a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'loss': [0.28902629017829895,\n",
       "  0.2202547937631607,\n",
       "  0.16039103269577026,\n",
       "  0.11016936600208282,\n",
       "  0.07140596956014633,\n",
       "  0.04495702683925629,\n",
       "  0.03243555128574371,\n",
       "  0.029452601447701454,\n",
       "  0.02737094834446907,\n",
       "  0.02553166262805462,\n",
       "  0.023521877825260162,\n",
       "  0.022156335413455963,\n",
       "  0.020837239921092987,\n",
       "  0.0194960106164217,\n",
       "  0.018492233008146286,\n",
       "  0.017453594133257866,\n",
       "  0.016792070120573044,\n",
       "  0.01623624935746193,\n",
       "  0.01587688736617565,\n",
       "  0.015483217313885689,\n",
       "  0.015154456719756126,\n",
       "  0.014814370311796665,\n",
       "  0.014546489343047142,\n",
       "  0.014186826534569263,\n",
       "  0.013880397193133831,\n",
       "  0.013571948744356632,\n",
       "  0.013269739225506783,\n",
       "  0.012982014566659927,\n",
       "  0.012676047161221504,\n",
       "  0.01239178329706192,\n",
       "  0.012107226997613907,\n",
       "  0.011835132725536823,\n",
       "  0.01155027188360691,\n",
       "  0.011313894763588905,\n",
       "  0.011007928289473057,\n",
       "  0.010770336724817753,\n",
       "  0.010514755733311176,\n",
       "  0.010272991843521595,\n",
       "  0.010068144649267197,\n",
       "  0.009839700534939766,\n",
       "  0.009651781991124153,\n",
       "  0.009456762112677097,\n",
       "  0.009281359612941742,\n",
       "  0.009107171557843685,\n",
       "  0.008949009701609612,\n",
       "  0.008775386027991772,\n",
       "  0.00862482376396656,\n",
       "  0.008499499410390854,\n",
       "  0.008317088708281517,\n",
       "  0.008161608129739761,\n",
       "  0.008027692325413227,\n",
       "  0.007869025692343712,\n",
       "  0.0077310726046562195,\n",
       "  0.007631789892911911,\n",
       "  0.007489981595426798,\n",
       "  0.007382088340818882,\n",
       "  0.007266689091920853,\n",
       "  0.007189505733549595,\n",
       "  0.007112841587513685,\n",
       "  0.007010759320110083,\n",
       "  0.006912137381732464,\n",
       "  0.006820018403232098,\n",
       "  0.006744593381881714,\n",
       "  0.006679668091237545,\n",
       "  0.006590605713427067,\n",
       "  0.006571875419467688,\n",
       "  0.00644448958337307,\n",
       "  0.006409012712538242,\n",
       "  0.006349032279103994,\n",
       "  0.006301097571849823,\n",
       "  0.006209980230778456,\n",
       "  0.0061779324896633625,\n",
       "  0.006105323787778616,\n",
       "  0.006058818660676479,\n",
       "  0.00602553877979517,\n",
       "  0.005987879820168018,\n",
       "  0.0059691863134503365,\n",
       "  0.005903114099055529,\n",
       "  0.005840995814651251,\n",
       "  0.005809095688164234,\n",
       "  0.005776914767920971,\n",
       "  0.005743207409977913,\n",
       "  0.005698307882994413,\n",
       "  0.005672551691532135,\n",
       "  0.005624252837151289,\n",
       "  0.005627801641821861,\n",
       "  0.005567138083279133,\n",
       "  0.00555016752332449,\n",
       "  0.005507873371243477,\n",
       "  0.0054921614937484264,\n",
       "  0.005452107172459364,\n",
       "  0.005446885712444782,\n",
       "  0.005421815440058708,\n",
       "  0.005391694139689207,\n",
       "  0.005370646715164185,\n",
       "  0.005337690934538841,\n",
       "  0.005320694297552109,\n",
       "  0.005316117778420448,\n",
       "  0.005299204494804144,\n",
       "  0.00525753665715456],\n",
       " 'val_accuracy': [0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.0],\n",
       " 'val_loss': [0.26862913370132446,\n",
       "  0.198458731174469,\n",
       "  0.13864576816558838,\n",
       "  0.09126082062721252,\n",
       "  0.055975060909986496,\n",
       "  0.03524487093091011,\n",
       "  0.030073143541812897,\n",
       "  0.02922418899834156,\n",
       "  0.025973862037062645,\n",
       "  0.022983368486166,\n",
       "  0.021351054310798645,\n",
       "  0.019825397059321404,\n",
       "  0.018438447266817093,\n",
       "  0.01724568009376526,\n",
       "  0.01617570035159588,\n",
       "  0.015428262762725353,\n",
       "  0.014814330264925957,\n",
       "  0.014334160834550858,\n",
       "  0.013928167521953583,\n",
       "  0.01356115285307169,\n",
       "  0.013218902051448822,\n",
       "  0.012913119047880173,\n",
       "  0.012592174112796783,\n",
       "  0.012318341992795467,\n",
       "  0.01204206794500351,\n",
       "  0.011791490018367767,\n",
       "  0.011536075733602047,\n",
       "  0.011251654475927353,\n",
       "  0.010993622243404388,\n",
       "  0.010737098753452301,\n",
       "  0.010501050390303135,\n",
       "  0.01024447288364172,\n",
       "  0.009980595670640469,\n",
       "  0.009677359834313393,\n",
       "  0.009483424946665764,\n",
       "  0.009229900315403938,\n",
       "  0.009065927006304264,\n",
       "  0.00887993536889553,\n",
       "  0.008762969635426998,\n",
       "  0.008534085005521774,\n",
       "  0.008362481370568275,\n",
       "  0.00824905838817358,\n",
       "  0.008158767595887184,\n",
       "  0.007994038984179497,\n",
       "  0.007892375811934471,\n",
       "  0.007799266837537289,\n",
       "  0.007736394181847572,\n",
       "  0.00771986972540617,\n",
       "  0.007525757886469364,\n",
       "  0.007464940659701824,\n",
       "  0.007413958199322224,\n",
       "  0.007492702454328537,\n",
       "  0.0073462664149701595,\n",
       "  0.007178710773587227,\n",
       "  0.0072412570007145405,\n",
       "  0.007205807603895664,\n",
       "  0.007154893130064011,\n",
       "  0.007021834142506123,\n",
       "  0.007066281978040934,\n",
       "  0.006898526102304459,\n",
       "  0.006874933373183012,\n",
       "  0.006873120553791523,\n",
       "  0.00683610001578927,\n",
       "  0.006785504519939423,\n",
       "  0.006751987151801586,\n",
       "  0.006578491535037756,\n",
       "  0.006710211746394634,\n",
       "  0.006700980011373758,\n",
       "  0.006445975974202156,\n",
       "  0.006440286990255117,\n",
       "  0.006551865488290787,\n",
       "  0.006504448130726814,\n",
       "  0.006387072615325451,\n",
       "  0.0062408652156591415,\n",
       "  0.006322353146970272,\n",
       "  0.006293995771557093,\n",
       "  0.0060643344186246395,\n",
       "  0.0062660579569637775,\n",
       "  0.006216713227331638,\n",
       "  0.006123104132711887,\n",
       "  0.006162453442811966,\n",
       "  0.006105566397309303,\n",
       "  0.005898331291973591,\n",
       "  0.005973076447844505,\n",
       "  0.006023915484547615,\n",
       "  0.006025438662618399,\n",
       "  0.0058434708043932915,\n",
       "  0.005801438353955746,\n",
       "  0.005904653109610081,\n",
       "  0.005904803052544594,\n",
       "  0.005769566632807255,\n",
       "  0.005706607364118099,\n",
       "  0.005834762938320637,\n",
       "  0.005711492151021957,\n",
       "  0.005659347400069237,\n",
       "  0.00580719206482172,\n",
       "  0.00570472190156579,\n",
       "  0.005581733770668507,\n",
       "  0.005753560923039913,\n",
       "  0.005575790069997311]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d3a9f814",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1fa3bbe8910>]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ad64c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388efa71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
