{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a4ab4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8cd98b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>396</td>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>397</td>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>398</td>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>399</td>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>400</td>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0             1        337          118                  4  4.5   4.5  9.65   \n",
       "1             2        324          107                  4  4.0   4.5  8.87   \n",
       "2             3        316          104                  3  3.0   3.5  8.00   \n",
       "3             4        322          110                  3  3.5   2.5  8.67   \n",
       "4             5        314          103                  2  2.0   3.0  8.21   \n",
       "..          ...        ...          ...                ...  ...   ...   ...   \n",
       "395         396        324          110                  3  3.5   3.5  9.04   \n",
       "396         397        325          107                  3  3.0   3.5  9.11   \n",
       "397         398        330          116                  4  5.0   4.5  9.45   \n",
       "398         399        312          103                  3  3.5   4.0  8.78   \n",
       "399         400        333          117                  4  5.0   4.0  9.66   \n",
       "\n",
       "     Research  Chance of Admit   \n",
       "0           1              0.92  \n",
       "1           1              0.76  \n",
       "2           1              0.72  \n",
       "3           1              0.80  \n",
       "4           0              0.65  \n",
       "..        ...               ...  \n",
       "395         1              0.82  \n",
       "396         1              0.84  \n",
       "397         1              0.91  \n",
       "398         0              0.67  \n",
       "399         1              0.95  \n",
       "\n",
       "[400 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('Admission_Predict.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbece7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 400 entries, 0 to 399\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         400 non-null    int64  \n",
      " 1   GRE Score          400 non-null    int64  \n",
      " 2   TOEFL Score        400 non-null    int64  \n",
      " 3   University Rating  400 non-null    int64  \n",
      " 4   SOP                400 non-null    float64\n",
      " 5   LOR                400 non-null    float64\n",
      " 6   CGPA               400 non-null    float64\n",
      " 7   Research           400 non-null    int64  \n",
      " 8   Chance of Admit    400 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 28.3 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c27e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['Serial No.'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6a84cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>324</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "      <td>0.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>325</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>330</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.45</td>\n",
       "      <td>1</td>\n",
       "      <td>0.91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.78</td>\n",
       "      <td>0</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>333</td>\n",
       "      <td>117</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.66</td>\n",
       "      <td>1</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0          337          118                  4  4.5   4.5  9.65         1   \n",
       "1          324          107                  4  4.0   4.5  8.87         1   \n",
       "2          316          104                  3  3.0   3.5  8.00         1   \n",
       "3          322          110                  3  3.5   2.5  8.67         1   \n",
       "4          314          103                  2  2.0   3.0  8.21         0   \n",
       "..         ...          ...                ...  ...   ...   ...       ...   \n",
       "395        324          110                  3  3.5   3.5  9.04         1   \n",
       "396        325          107                  3  3.0   3.5  9.11         1   \n",
       "397        330          116                  4  5.0   4.5  9.45         1   \n",
       "398        312          103                  3  3.5   4.0  8.78         0   \n",
       "399        333          117                  4  5.0   4.0  9.66         1   \n",
       "\n",
       "     Chance of Admit   \n",
       "0                0.92  \n",
       "1                0.76  \n",
       "2                0.72  \n",
       "3                0.80  \n",
       "4                0.65  \n",
       "..                ...  \n",
       "395              0.82  \n",
       "396              0.84  \n",
       "397              0.91  \n",
       "398              0.67  \n",
       "399              0.95  \n",
       "\n",
       "[400 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a636ef47",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df.iloc[:,0:-1].values\n",
    "Y=df.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68076da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.92, 0.76, 0.72, 0.8 , 0.65, 0.9 , 0.75, 0.68, 0.5 , 0.45, 0.52,\n",
       "       0.84, 0.78, 0.62, 0.61, 0.54, 0.66, 0.65, 0.63, 0.62, 0.64, 0.7 ,\n",
       "       0.94, 0.95, 0.97, 0.94, 0.76, 0.44, 0.46, 0.54, 0.65, 0.74, 0.91,\n",
       "       0.9 , 0.94, 0.88, 0.64, 0.58, 0.52, 0.48, 0.46, 0.49, 0.53, 0.87,\n",
       "       0.91, 0.88, 0.86, 0.89, 0.82, 0.78, 0.76, 0.56, 0.78, 0.72, 0.7 ,\n",
       "       0.64, 0.64, 0.46, 0.36, 0.42, 0.48, 0.47, 0.54, 0.56, 0.52, 0.55,\n",
       "       0.61, 0.57, 0.68, 0.78, 0.94, 0.96, 0.93, 0.84, 0.74, 0.72, 0.74,\n",
       "       0.64, 0.44, 0.46, 0.5 , 0.96, 0.92, 0.92, 0.94, 0.76, 0.72, 0.66,\n",
       "       0.64, 0.74, 0.64, 0.38, 0.34, 0.44, 0.36, 0.42, 0.48, 0.86, 0.9 ,\n",
       "       0.79, 0.71, 0.64, 0.62, 0.57, 0.74, 0.69, 0.87, 0.91, 0.93, 0.68,\n",
       "       0.61, 0.69, 0.62, 0.72, 0.59, 0.66, 0.56, 0.45, 0.47, 0.71, 0.94,\n",
       "       0.94, 0.57, 0.61, 0.57, 0.64, 0.85, 0.78, 0.84, 0.92, 0.96, 0.77,\n",
       "       0.71, 0.79, 0.89, 0.82, 0.76, 0.71, 0.8 , 0.78, 0.84, 0.9 , 0.92,\n",
       "       0.97, 0.8 , 0.81, 0.75, 0.83, 0.96, 0.79, 0.93, 0.94, 0.86, 0.79,\n",
       "       0.8 , 0.77, 0.7 , 0.65, 0.61, 0.52, 0.57, 0.53, 0.67, 0.68, 0.81,\n",
       "       0.78, 0.65, 0.64, 0.64, 0.65, 0.68, 0.89, 0.86, 0.89, 0.87, 0.85,\n",
       "       0.9 , 0.82, 0.72, 0.73, 0.71, 0.71, 0.68, 0.75, 0.72, 0.89, 0.84,\n",
       "       0.93, 0.93, 0.88, 0.9 , 0.87, 0.86, 0.94, 0.77, 0.78, 0.73, 0.73,\n",
       "       0.7 , 0.72, 0.73, 0.72, 0.97, 0.97, 0.69, 0.57, 0.63, 0.66, 0.64,\n",
       "       0.68, 0.79, 0.82, 0.95, 0.96, 0.94, 0.93, 0.91, 0.85, 0.84, 0.74,\n",
       "       0.76, 0.75, 0.76, 0.71, 0.67, 0.61, 0.63, 0.64, 0.71, 0.82, 0.73,\n",
       "       0.74, 0.69, 0.64, 0.91, 0.88, 0.85, 0.86, 0.7 , 0.59, 0.6 , 0.65,\n",
       "       0.7 , 0.76, 0.63, 0.81, 0.72, 0.71, 0.8 , 0.77, 0.74, 0.7 , 0.71,\n",
       "       0.93, 0.85, 0.79, 0.76, 0.78, 0.77, 0.9 , 0.87, 0.71, 0.7 , 0.7 ,\n",
       "       0.75, 0.71, 0.72, 0.73, 0.83, 0.77, 0.72, 0.54, 0.49, 0.52, 0.58,\n",
       "       0.78, 0.89, 0.7 , 0.66, 0.67, 0.68, 0.8 , 0.81, 0.8 , 0.94, 0.93,\n",
       "       0.92, 0.89, 0.82, 0.79, 0.58, 0.56, 0.56, 0.64, 0.61, 0.68, 0.76,\n",
       "       0.86, 0.9 , 0.71, 0.62, 0.66, 0.65, 0.73, 0.62, 0.74, 0.79, 0.8 ,\n",
       "       0.69, 0.7 , 0.76, 0.84, 0.78, 0.67, 0.66, 0.65, 0.54, 0.58, 0.79,\n",
       "       0.8 , 0.75, 0.73, 0.72, 0.62, 0.67, 0.81, 0.63, 0.69, 0.8 , 0.43,\n",
       "       0.8 , 0.73, 0.75, 0.71, 0.73, 0.83, 0.72, 0.94, 0.81, 0.81, 0.75,\n",
       "       0.79, 0.58, 0.59, 0.47, 0.49, 0.47, 0.42, 0.57, 0.62, 0.74, 0.73,\n",
       "       0.64, 0.63, 0.59, 0.73, 0.79, 0.68, 0.7 , 0.81, 0.85, 0.93, 0.91,\n",
       "       0.69, 0.77, 0.86, 0.74, 0.57, 0.51, 0.67, 0.72, 0.89, 0.95, 0.79,\n",
       "       0.39, 0.38, 0.34, 0.47, 0.56, 0.71, 0.78, 0.73, 0.82, 0.62, 0.96,\n",
       "       0.96, 0.46, 0.53, 0.49, 0.76, 0.64, 0.71, 0.84, 0.77, 0.89, 0.82,\n",
       "       0.84, 0.91, 0.67, 0.95])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3c18549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[337.  , 118.  ,   4.  , ...,   4.5 ,   9.65,   1.  ],\n",
       "       [324.  , 107.  ,   4.  , ...,   4.5 ,   8.87,   1.  ],\n",
       "       [316.  , 104.  ,   3.  , ...,   3.5 ,   8.  ,   1.  ],\n",
       "       ...,\n",
       "       [330.  , 116.  ,   4.  , ...,   4.5 ,   9.45,   1.  ],\n",
       "       [312.  , 103.  ,   3.  , ...,   4.  ,   8.78,   0.  ],\n",
       "       [333.  , 117.  ,   4.  , ...,   4.  ,   9.66,   1.  ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b915762",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49d086b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "449c283c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.76210664,  1.74697064,  0.79882862, ...,  1.16732114,\n",
       "         1.76481828,  0.90911166],\n",
       "       [ 0.62765641, -0.06763531,  0.79882862, ...,  1.16732114,\n",
       "         0.45515126,  0.90911166],\n",
       "       [-0.07046681, -0.56252785, -0.07660001, ...,  0.05293342,\n",
       "        -1.00563118,  0.90911166],\n",
       "       ...,\n",
       "       [ 1.15124883,  1.41704229,  0.79882862, ...,  1.16732114,\n",
       "         1.42900622,  0.90911166],\n",
       "       [-0.41952842, -0.72749202, -0.07660001, ...,  0.61012728,\n",
       "         0.30403584, -1.09997489],\n",
       "       [ 1.41304503,  1.58200646,  0.79882862, ...,  0.61012728,\n",
       "         1.78160888,  0.90911166]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a5c895c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test= train_test_split(X,Y,test_size=0.1,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab7667c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in f:\\okok\\lib\\site-packages (2.17.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.17.0 in f:\\okok\\lib\\site-packages (from tensorflow) (2.17.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.4.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (23.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.25.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.29.0)\n",
      "Requirement already satisfied: setuptools in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (67.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (4.6.3)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.65.4)\n",
      "Requirement already satisfied: tensorboard<2.18,>=2.17 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (2.17.0)\n",
      "Requirement already satisfied: keras>=3.2.0 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in f:\\okok\\lib\\site-packages (from tensorflow-intel==2.17.0->tensorflow) (1.24.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in f:\\okok\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.17.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: rich in f:\\okok\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in f:\\okok\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in f:\\okok\\lib\\site-packages (from keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.12.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in f:\\okok\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in f:\\okok\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\okok\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in f:\\okok\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.17.0->tensorflow) (2023.5.7)\n",
      "Requirement already satisfied: markdown>=2.6.8 in f:\\okok\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in f:\\okok\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in f:\\okok\\lib\\site-packages (from tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in f:\\okok\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow-intel==2.17.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in f:\\okok\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in f:\\okok\\lib\\site-packages (from rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in f:\\okok\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow-intel==2.17.0->tensorflow) (0.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6744a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "81987c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1df3d732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 7)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "#these 7 columns will be the input layer of this nn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "82088a0d",
   "metadata": {},
   "source": [
    "i/p layer          hidden              output layer\n",
    "(7 i/p)          layer (having 3\n",
    "                 perceptrons)\n",
    ".                  \n",
    ".                   .\n",
    ".\n",
    ".                   .                     .\n",
    ".\n",
    ".                   .\n",
    ".\n",
    "7*3=21 bytes     3 bias                  1 bias\n",
    "                 21+3=24 parameters     24+3+1=28 parameters\n",
    "                 \n",
    "                 \n",
    "### therefore there are total 28 trainable parameters in our neural network            \n",
    "                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7ffde5ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\okok\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(7,activation='relu',input_dim=X_train.shape[1])) #for hidden layer...3 perceptrons\n",
    "model.add(Dense(3,activation='relu'))\n",
    "# model.add(Dense(1,activation='relu'))\n",
    "model.add(Dense(1,activation='linear')) #for o/p layer\n",
    "\n",
    "#Dense: Dense(3, activation='relu', input_dim=X_train.shape[1]): This creates a dense (fully connected) layer with 3 neurons (perceptrons).\n",
    "#activation='relu': activation='relu': The activation function used is ReLU (Rectified Linear Unit), which introduces non-linearity to the model. ReLU outputs the input directly if it is positive; otherwise, it outputs zero.\n",
    "#input_dim=X_train.shape[1]: This specifies the input dimension to the layer, which is the number of features in the training data X_train.\n",
    "#Dense(3, activation='linear'): This creates another dense layer with 3 neurons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d183dae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_10\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_10\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │              <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)                   │              \u001b[38;5;34m56\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │              \u001b[38;5;34m24\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)                   │               \u001b[38;5;34m4\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84</span> (336.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m84\u001b[0m (336.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">84</span> (336.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m84\u001b[0m (336.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ee463979",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"Adam\",loss='mean_squared_error')\n",
    "#optimizer=\"Adam\": This specifies the optimizer to use for training the model. Adam (short for Adaptive Moment Estimation) is a popular optimization algorithm that combines the advantages of two other extensions of stochastic gradient descent: AdaGrad and RMSProp. It is efficient and works well with large datasets and high-dimensional parameter spaces.\n",
    "# Optimizer: Adam helps in adjusting the weights of the network to minimize the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "89bea065",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - loss: 2.8526 - val_loss: 2.0227\n",
      "Epoch 2/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.6969 - val_loss: 1.2463\n",
      "Epoch 3/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1.0703 - val_loss: 0.8406\n",
      "Epoch 4/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.7308 - val_loss: 0.6212\n",
      "Epoch 5/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.6166 - val_loss: 0.4885\n",
      "Epoch 6/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4771 - val_loss: 0.4012\n",
      "Epoch 7/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.4149 - val_loss: 0.3351\n",
      "Epoch 8/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2997 - val_loss: 0.2817\n",
      "Epoch 9/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.2567 - val_loss: 0.2357\n",
      "Epoch 10/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.2175 - val_loss: 0.1957\n",
      "Epoch 11/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2011 - val_loss: 0.1621\n",
      "Epoch 12/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1481 - val_loss: 0.1322\n",
      "Epoch 13/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1462 - val_loss: 0.1054\n",
      "Epoch 14/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.1074 - val_loss: 0.0851\n",
      "Epoch 15/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0858 - val_loss: 0.0704\n",
      "Epoch 16/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0820 - val_loss: 0.0603\n",
      "Epoch 17/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0558 - val_loss: 0.0532\n",
      "Epoch 18/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0563 - val_loss: 0.0480\n",
      "Epoch 19/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0533 - val_loss: 0.0439\n",
      "Epoch 20/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0487 - val_loss: 0.0411\n",
      "Epoch 21/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0452 - val_loss: 0.0387\n",
      "Epoch 22/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0414 - val_loss: 0.0365\n",
      "Epoch 23/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0360 - val_loss: 0.0346\n",
      "Epoch 24/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0377 - val_loss: 0.0326\n",
      "Epoch 25/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0361 - val_loss: 0.0311\n",
      "Epoch 26/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0318 - val_loss: 0.0298\n",
      "Epoch 27/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0326 - val_loss: 0.0284\n",
      "Epoch 28/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0326 - val_loss: 0.0273\n",
      "Epoch 29/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0274 - val_loss: 0.0265\n",
      "Epoch 30/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0315 - val_loss: 0.0252\n",
      "Epoch 31/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0326 - val_loss: 0.0244\n",
      "Epoch 32/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0270 - val_loss: 0.0236\n",
      "Epoch 33/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0263 - val_loss: 0.0228\n",
      "Epoch 34/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0289 - val_loss: 0.0223\n",
      "Epoch 35/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0208 - val_loss: 0.0214\n",
      "Epoch 36/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0219 - val_loss: 0.0209\n",
      "Epoch 37/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0180 - val_loss: 0.0203\n",
      "Epoch 38/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0210 - val_loss: 0.0199\n",
      "Epoch 39/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0175 - val_loss: 0.0193\n",
      "Epoch 40/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0189 - val_loss: 0.0187\n",
      "Epoch 41/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 42/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0188 - val_loss: 0.0180\n",
      "Epoch 43/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0168 - val_loss: 0.0176\n",
      "Epoch 44/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0182 - val_loss: 0.0172\n",
      "Epoch 45/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0160 - val_loss: 0.0169\n",
      "Epoch 46/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0151 - val_loss: 0.0166\n",
      "Epoch 47/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0183 - val_loss: 0.0162\n",
      "Epoch 48/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0156 - val_loss: 0.0159\n",
      "Epoch 49/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0129 - val_loss: 0.0156\n",
      "Epoch 50/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0144 - val_loss: 0.0152\n",
      "Epoch 51/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0128 - val_loss: 0.0150\n",
      "Epoch 52/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0146 - val_loss: 0.0148\n",
      "Epoch 53/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0135 - val_loss: 0.0146\n",
      "Epoch 54/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 55/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0119 - val_loss: 0.0139\n",
      "Epoch 56/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0131 - val_loss: 0.0138\n",
      "Epoch 57/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0128 - val_loss: 0.0135\n",
      "Epoch 58/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0140 - val_loss: 0.0133\n",
      "Epoch 59/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 0.0123 - val_loss: 0.0131\n",
      "Epoch 60/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0129 - val_loss: 0.0129\n",
      "Epoch 61/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0102 - val_loss: 0.0128\n",
      "Epoch 62/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0111 - val_loss: 0.0125\n",
      "Epoch 63/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0129 - val_loss: 0.0124\n",
      "Epoch 64/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0106 - val_loss: 0.0122\n",
      "Epoch 65/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0101 - val_loss: 0.0120\n",
      "Epoch 66/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0103 - val_loss: 0.0119\n",
      "Epoch 67/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0091 - val_loss: 0.0118\n",
      "Epoch 68/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0091 - val_loss: 0.0117\n",
      "Epoch 69/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0114 - val_loss: 0.0114\n",
      "Epoch 70/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0095 - val_loss: 0.0113\n",
      "Epoch 71/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0112\n",
      "Epoch 72/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0074 - val_loss: 0.0111\n",
      "Epoch 73/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0092 - val_loss: 0.0109\n",
      "Epoch 74/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0079 - val_loss: 0.0108\n",
      "Epoch 75/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0107\n",
      "Epoch 76/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0107 - val_loss: 0.0106\n",
      "Epoch 77/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0092 - val_loss: 0.0105\n",
      "Epoch 78/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0102 - val_loss: 0.0104\n",
      "Epoch 79/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0080 - val_loss: 0.0103\n",
      "Epoch 80/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0091 - val_loss: 0.0101\n",
      "Epoch 81/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0094 - val_loss: 0.0100\n",
      "Epoch 82/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0092 - val_loss: 0.0099\n",
      "Epoch 83/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0098\n",
      "Epoch 84/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0089 - val_loss: 0.0097\n",
      "Epoch 85/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0071 - val_loss: 0.0096\n",
      "Epoch 86/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0086 - val_loss: 0.0095\n",
      "Epoch 87/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0089 - val_loss: 0.0094\n",
      "Epoch 88/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0072 - val_loss: 0.0093\n",
      "Epoch 89/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0077 - val_loss: 0.0092\n",
      "Epoch 90/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0077 - val_loss: 0.0091\n",
      "Epoch 91/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0071 - val_loss: 0.0091\n",
      "Epoch 92/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0069 - val_loss: 0.0089\n",
      "Epoch 93/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0074 - val_loss: 0.0089\n",
      "Epoch 94/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0077 - val_loss: 0.0087\n",
      "Epoch 95/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0063 - val_loss: 0.0087\n",
      "Epoch 96/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0069 - val_loss: 0.0086\n",
      "Epoch 97/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0059 - val_loss: 0.0085\n",
      "Epoch 98/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0060 - val_loss: 0.0084\n",
      "Epoch 99/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0070 - val_loss: 0.0083\n",
      "Epoch 100/100\n",
      "\u001b[1m29/29\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0076 - val_loss: 0.0082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f1b4178710>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history=model.fit(X_train,Y_train,epochs=100,batch_size=10,verbose=1,validation_split=0.2)\n",
    "# validation split is the technique where the train data is also further more splitted into train and test data\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "c3a43d1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 81ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d4d7ab99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7132774649909306"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "r2_score(Y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e773a96",
   "metadata": {},
   "source": [
    "this gives the poorest result,hence we will now try to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "9ffd6f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation and improvements\n",
    "# increase the no. of epochs\n",
    "# increase nodes in the hidden layer\n",
    "# add more layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "1523b144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [2.597830295562744,\n",
       "  1.5516432523727417,\n",
       "  1.0164180994033813,\n",
       "  0.727935791015625,\n",
       "  0.5607693195343018,\n",
       "  0.4458787739276886,\n",
       "  0.36478811502456665,\n",
       "  0.3024195432662964,\n",
       "  0.25206565856933594,\n",
       "  0.21082620322704315,\n",
       "  0.17475451529026031,\n",
       "  0.1441897749900818,\n",
       "  0.11887718737125397,\n",
       "  0.09843815118074417,\n",
       "  0.08331535756587982,\n",
       "  0.07224161177873611,\n",
       "  0.06386806070804596,\n",
       "  0.05765075236558914,\n",
       "  0.05281596630811691,\n",
       "  0.048696354031562805,\n",
       "  0.04535333439707756,\n",
       "  0.04217606782913208,\n",
       "  0.03949384763836861,\n",
       "  0.03722342848777771,\n",
       "  0.03490491211414337,\n",
       "  0.03289122134447098,\n",
       "  0.03114173747599125,\n",
       "  0.02950168587267399,\n",
       "  0.02813916653394699,\n",
       "  0.0268402099609375,\n",
       "  0.025700148195028305,\n",
       "  0.02466428652405739,\n",
       "  0.023782966658473015,\n",
       "  0.022937791422009468,\n",
       "  0.02211097627878189,\n",
       "  0.021348493173718452,\n",
       "  0.020638776943087578,\n",
       "  0.02001049742102623,\n",
       "  0.01936138980090618,\n",
       "  0.018851839005947113,\n",
       "  0.01829857937991619,\n",
       "  0.017738278955221176,\n",
       "  0.017259107902646065,\n",
       "  0.016861766576766968,\n",
       "  0.0163694079965353,\n",
       "  0.015960639342665672,\n",
       "  0.015579674392938614,\n",
       "  0.015229985117912292,\n",
       "  0.014886726625263691,\n",
       "  0.014614143408834934,\n",
       "  0.0142202228307724,\n",
       "  0.013937166891992092,\n",
       "  0.013681229203939438,\n",
       "  0.01332112681120634,\n",
       "  0.013065841980278492,\n",
       "  0.012841892428696156,\n",
       "  0.012572165578603745,\n",
       "  0.012336129322648048,\n",
       "  0.012082350440323353,\n",
       "  0.011881939135491848,\n",
       "  0.01169373095035553,\n",
       "  0.011479237116873264,\n",
       "  0.011253582313656807,\n",
       "  0.011082297191023827,\n",
       "  0.010870760306715965,\n",
       "  0.010789932683110237,\n",
       "  0.010521430522203445,\n",
       "  0.010413740761578083,\n",
       "  0.01021817047148943,\n",
       "  0.01005704328417778,\n",
       "  0.009885596111416817,\n",
       "  0.009785371832549572,\n",
       "  0.009606907144188881,\n",
       "  0.009447780437767506,\n",
       "  0.009335040114820004,\n",
       "  0.009182157926261425,\n",
       "  0.009042755700647831,\n",
       "  0.008933174423873425,\n",
       "  0.008802304044365883,\n",
       "  0.00872140470892191,\n",
       "  0.00858000386506319,\n",
       "  0.008479880169034004,\n",
       "  0.008370600640773773,\n",
       "  0.008270390331745148,\n",
       "  0.008221819065511227,\n",
       "  0.008068155497312546,\n",
       "  0.007964108139276505,\n",
       "  0.007891473360359669,\n",
       "  0.007829317823052406,\n",
       "  0.007699755951762199,\n",
       "  0.007622516714036465,\n",
       "  0.007542090956121683,\n",
       "  0.007482186891138554,\n",
       "  0.00739650335162878,\n",
       "  0.007289401255548,\n",
       "  0.0072205387987196445,\n",
       "  0.00711597315967083,\n",
       "  0.00705498643219471,\n",
       "  0.006982192397117615,\n",
       "  0.006881138309836388],\n",
       " 'val_loss': [2.0226545333862305,\n",
       "  1.2462608814239502,\n",
       "  0.8406232595443726,\n",
       "  0.6212305426597595,\n",
       "  0.4884597361087799,\n",
       "  0.40122777223587036,\n",
       "  0.3351329565048218,\n",
       "  0.28172600269317627,\n",
       "  0.23568055033683777,\n",
       "  0.1957017183303833,\n",
       "  0.162055104970932,\n",
       "  0.13216367363929749,\n",
       "  0.10535163432359695,\n",
       "  0.08510813117027283,\n",
       "  0.07041649520397186,\n",
       "  0.06030917912721634,\n",
       "  0.0532136969268322,\n",
       "  0.047985758632421494,\n",
       "  0.04385024681687355,\n",
       "  0.04107941687107086,\n",
       "  0.03867834806442261,\n",
       "  0.036497943103313446,\n",
       "  0.03460216149687767,\n",
       "  0.03260783851146698,\n",
       "  0.031148530542850494,\n",
       "  0.02982509136199951,\n",
       "  0.028353426605463028,\n",
       "  0.02725876122713089,\n",
       "  0.026474881917238235,\n",
       "  0.02522379718720913,\n",
       "  0.024405354633927345,\n",
       "  0.02357311360538006,\n",
       "  0.02279803901910782,\n",
       "  0.02229195646941662,\n",
       "  0.021395279094576836,\n",
       "  0.020871922373771667,\n",
       "  0.02032635547220707,\n",
       "  0.019853441044688225,\n",
       "  0.019323544576764107,\n",
       "  0.01874992437660694,\n",
       "  0.01845516823232174,\n",
       "  0.018006697297096252,\n",
       "  0.017614703625440598,\n",
       "  0.01716589368879795,\n",
       "  0.016878925263881683,\n",
       "  0.01663997396826744,\n",
       "  0.01621069200336933,\n",
       "  0.0159181859344244,\n",
       "  0.015624946914613247,\n",
       "  0.01524941436946392,\n",
       "  0.015035348013043404,\n",
       "  0.014817620627582073,\n",
       "  0.014559328556060791,\n",
       "  0.014155724085867405,\n",
       "  0.013903011567890644,\n",
       "  0.013789873570203781,\n",
       "  0.013482659123837948,\n",
       "  0.013344250619411469,\n",
       "  0.013124330900609493,\n",
       "  0.012892567552626133,\n",
       "  0.0127664590254426,\n",
       "  0.012488149106502533,\n",
       "  0.012396108359098434,\n",
       "  0.012233633548021317,\n",
       "  0.012005267664790154,\n",
       "  0.011855780147016048,\n",
       "  0.011763973161578178,\n",
       "  0.011687234975397587,\n",
       "  0.011418499983847141,\n",
       "  0.0113199632614851,\n",
       "  0.011173789389431477,\n",
       "  0.011100539937615395,\n",
       "  0.010873897932469845,\n",
       "  0.010806935839354992,\n",
       "  0.01073784101754427,\n",
       "  0.010569419711828232,\n",
       "  0.01046142727136612,\n",
       "  0.01036569382995367,\n",
       "  0.010273666121065617,\n",
       "  0.010103191249072552,\n",
       "  0.010019661858677864,\n",
       "  0.009899205528199673,\n",
       "  0.009806174784898758,\n",
       "  0.009675015695393085,\n",
       "  0.009578753262758255,\n",
       "  0.009494539350271225,\n",
       "  0.009388714097440243,\n",
       "  0.009291954338550568,\n",
       "  0.00918746367096901,\n",
       "  0.009095490910112858,\n",
       "  0.009056128561496735,\n",
       "  0.008929581381380558,\n",
       "  0.008882157504558563,\n",
       "  0.00873849168419838,\n",
       "  0.008672581054270267,\n",
       "  0.008625692687928677,\n",
       "  0.00850625615566969,\n",
       "  0.008405227214097977,\n",
       "  0.008321866393089294,\n",
       "  0.008222144097089767]}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.history\n",
    "#.history in the syntax to get the dictionary of loss and validation value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "3440223d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f1b5531e50>]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the above\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "# xlabel=no. of epoches\n",
    "\n",
    "# this helps in identify the overfitting due to loss or val_loss on the basis of epoches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "382d79e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
